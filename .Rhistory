get_model_metrics <- function(model_obj, data_series, cutoff=c(2018,12)) {
# Split data
train_data <- window(data_series, end=cutoff)
actual_2019 <- window(data_series, start=cutoff+c(0,1), end=cutoff+c(1,0))
# Extract structure from the input model object
orders <- model_obj$arma[c(1, 6, 2)] # p, d, q
season <- model_obj$arma[c(3, 7, 4)] # P, D, Q
period <- model_obj$arma[5]
# Re-estimate on training data
mod_train <- arima(log(train_data), order=orders, seasonal=list(order=season, period=period))
pred <- predict(mod_train, n.ahead=12)
# Forecast values (Back-transformed)
fc_mean <- exp(pred$pred)
fc_low  <- exp(pred$pred - 1.96 * pred$se)
fc_high <- exp(pred$pred + 1.96 * pred$se)
# Calculate Metrics
rmse  <- sqrt(mean((actual_2019 - fc_mean)^2))
mae   <- mean(abs(actual_2019 - fc_mean))
rmspe <- sqrt(mean(((actual_2019 - fc_mean)/actual_2019)^2))
mape  <- mean(abs((actual_2019 - fc_mean)/actual_2019)) * 100
mean_len_ci <- mean(fc_high - fc_low)
return(c(RMSE=rmse, MAE=mae, RMSPE=rmspe, MAPE=mape, MeanCI=mean_len_ci))
}
# --- 2. Calculate Scores ---
# Ensure mod1 and mod2 are the correct models you want to compare
m1_scores <- get_model_metrics(mod1, serie)
m2_scores <- get_model_metrics(mod2, serie)
# --- 3. Create Dataframe (Numeric Part First) ---
resul_num <- data.frame(
Parameters = c(length(coef(mod1)), length(coef(mod2))),
Sigma2     = c(mod1$sigma2, mod2$sigma2),
AIC        = c(AIC(mod1), AIC(mod2)),
BIC        = c(BIC(mod1), BIC(mod2)),
RMSE       = c(m1_scores["RMSE"], m2_scores["RMSE"]),
MAE        = c(m1_scores["MAE"], m2_scores["MAE"]),
RMSPE      = c(m1_scores["RMSPE"], m2_scores["RMSPE"]),
MAPE       = c(m1_scores["MAPE"], m2_scores["MAPE"]),
Mean_CI    = c(m1_scores["MeanCI"], m2_scores["MeanCI"])
)
# --- 4. Round Numbers to Strings ---
# This makes the table pretty (e.g., "1.57" instead of "1.5738292")
resul_clean <- format(round(resul_num, 4), nsmall=4)
# --- 5. Add Text Columns & Row Names ---
resul_clean$Homoscedasticity <- c("Validated", "Validated")
resul_clean$Normality        <- c("Validated", "Validated")
resul_clean$Independence     <- c("Fail (<0.05)", "Fail (<0.05)")
resul_clean$Causality        <- c("Validated", "Validated")
resul_clean$Invertibility    <- c("Validated", "Validated")
resul_clean$Stability        <- c("Validated", "Validated")
resul_clean$Predictive_capability <- c("Validated", "Validated")
# Set Model Names
row.names(resul_clean) <- c("Model 1: (2,0,0)(0,1,3)12", "Model 2: (2,0,0)(3,1,0)12")
# --- 6. Pivot (Transpose) ---
# This flips the table and converts it to a matrix of strings
final_table <- t(resul_clean)
# Print the final result
print(final_table, quote=FALSE)
# --- Section 4: Final Prediction (Model 1) ---
# Model: ARIMA(2,0,0)(0,1,3)12
pdq <- c(2, 0, 0)
PDQ <- c(0, 1, 3)
# 1. Estimate on the FULL available history (1990-2019)
# We use the log-transformed series 'lnserie'
final_mod <- arima(lnserie,
order = pdq,
seasonal = list(order = PDQ, period = 12))
cat("\n--- Final Model Estimation (Full History) ---\n")
print(final_mod)
# 2. Forecast the Future (12 months ahead)
# This predicts Jan-Dec 2020
future_pred <- predict(final_mod, n.ahead = 12)
# 3. Process the Results
# Get the end date of your current series
last_date <- end(serie)
# Create Time Series for the Forecast (Log Scale)
pr <- ts(future_pred$pred, start = last_date + c(0, 1), freq = 12)
se <- ts(future_pred$se, start = last_date + c(0, 1), freq = 12)
# Back-transform to Original Scale (GwH) using exp()
pred_mean <- exp(pr)                # Point Forecast
pred_low  <- exp(pr - 1.96 * se)    # Lower 95% Confidence Interval
pred_high <- exp(pr + 1.96 * se)    # Upper 95% Confidence Interval
# 4. Visualization
# We plot the whole series + forecast, zooming in on the last 5 years
ts.plot(serie, pred_mean, pred_low, pred_high,
lty = c(1, 1, 2, 2),
col = c("black", "red", "blue", "blue"),
xlim = c(2015, 2021),  # Zoom to see details
ylab = "GwH",
main = "Final Forecast 2020: ARIMA(2,0,0)(0,1,3)12")
# Add grid and legend
abline(v = 2015:2021, lty = 3, col = "gray")
legend("topleft", legend = c("Historical Data", "Forecast", "95% CI"),
col = c("black", "red", "blue"), lty = c(1, 1, 2), bty = "n", cex=0.8)
# 5. Print the Numerical Forecasts (Table for Report)
cat("\n--- Forecast Values (GwH) for 2020 ---\n")
forecast_table <- data.frame(
Month = time(pred_mean),
Lower_95 = round(pred_low, 2),
Forecast = round(pred_mean, 2),
Upper_95 = round(pred_high, 2)
)
print(forecast_table)
knitr::opts_chunk$set(echo = TRUE)
serie=window(ts(read.table("ConsumElec.dat"),start=1990,freq=12))
plot(serie, main="Gross internal consumption of electrical energy in Spain", ylab="GwH")
abline(v=1985:2020,lty=3,col=4)
# Analyze Variance - Adapted from Session1.rmd
# We group by year (12 months) to check mean vs variance
groupedserie <- matrix(serie, nrow=12)
m <- apply(groupedserie, 2, mean)
v <- apply(groupedserie, 2, var)
plot(v~m, main="Mean-Variance Plot")
abline(lm(v~m), col=2, lty=3)
lnserie = log(serie)
plot(lnserie, main="Log-Transformed Series")
groupedlnserie <- matrix(lnserie, nrow=12)
m <- apply(groupedlnserie, 2, mean)
v <- apply(groupedlnserie, 2, var)
plot(v~m, main="Mean-Variance Plot")
abline(lm(v~m), col=2, lty=3)
# 1. Apply Seasonal Difference (removes yearly pattern)
d12lnserie <- diff(lnserie, lag=12)
# 2. Apply Regular Difference on top (removes trend)
d1d12lnserie <- diff(d12lnserie)
cat("Variance of d12 (Seasonal only):", var(d12lnserie, na.rm=TRUE), "\n")
cat("Variance of d1d12 (Seasonal + Regular):", var(d1d12lnserie, na.rm=TRUE), "\n")
par(mfrow=c(1,2))
acf(d12lnserie,ylim=c(-1,1),col=c(2,rep(1,11)),lwd=2,lag.max=72, main="ACF of d12lnserie")
pacf(d12lnserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=72, main="PACF of d12lnserie")
par(mfrow=c(1,1))
# --- Step 2: Estimation ---
# Model 1: ARIMA(2, 0, 0)(0, 1, 3)12
# This implies: (1 - phi1*B) * (1 - B^12) * log(Xt) = (1 + Theta1*B^12) * Zt
mod1 <- arima(lnserie,
order = c(2, 0, 0),
seasonal = list(order = c(0, 1, 3), period = 12))
# Model 2: ARIMA(2, 0, 0)(3, 1, 0)12
# This tests if adding a second AR lag improves the fit
mod2 <- arima(lnserie,
order = c(2, 0, 0),
seasonal = list(order = c(3, 1, 0), period = 12))
#Function to print neat results
model_estimation <- function(model, name) {
cat("\n================ ", name, " ================\n")
print(model)
# Calculate T-ratios: Coefficient / Standard Error
t_ratios <- model$coef / sqrt(diag(model$var.coef))
# Create a summary table
results <- data.frame(
Coeff = round(model$coef, 4),
SE = round(sqrt(diag(model$var.coef)), 4),
t_ratio = round(t_ratios, 4),
Significant = abs(t_ratios) > 2 # Significance threshold
)
print(results)
cat("\nSigma^2 (Variance of Residuals):", model$sigma2)
cat("\nAIC:", AIC(model))
cat("\nBIC:", BIC(model))
}
model_estimation(mod1,"ARIMA(2, 0, 0)(0, 1, 3)12")
model_estimation(mod2,"ARIMA(2, 0, 0)(3, 1, 0)12")
#################Validation#################################
validation=function(model){
s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(2,2),mar=c(3,3,3,3))
#Residuals plot
plot(resid,main="Residuals")
abline(h=0)
abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
#Square Root of absolute values of residuals (Homocedasticity)
scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals",
lpars=list(col=2))
#Normal plot of residuals
qqnorm(resid)
qqline(resid,col=2,lwd=2)
##Histogram of residuals with normal curve
hist(resid,breaks=20,freq=FALSE)
curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)
#ACF & PACF of Square residuals
par(mfrow=c(1,2))
# ACF of resid^2
acf(resid^2,
ylim=c(-1,1),
lag.max=60,
col=c(2, rep(1, s-1)),
lwd=1,
main="ACF of Squared Residuals")
# PACF of resid^2
pacf(resid^2,
ylim=c(-1,1),
lag.max=60,
col=c(rep(1, s-1), 2),
lwd=1,
main="PACF of Squared Residuals")
par(mfrow=c(1,1))
#ACF & PACF of residuals
par(mfrow=c(1,2))
acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
par(mfrow=c(1,1))
#Ljung-Box p-values
par(mar=c(2,2,1,1))
tsdiag(model,gof.lag=7*s)
cat("\n--------------------------------------------------------------------\n")
print(model)
#Stationary and Invertible
cat("\nModul of AR Characteristic polynomial Roots: ",
Mod(polyroot(c(1,-model$model$phi))),"\n")
cat("\nModul of MA Characteristic polynomial Roots: ",
Mod(polyroot(c(1,model$model$theta))),"\n")
suppressMessages(require(forecast,quietly=TRUE,warn.conflicts=FALSE))
plot(model)
#Model expressed as an MA infinity (psi-weights)
psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
names(psis)=paste("psi",1:36)
cat("\nPsi-weights (MA(inf))\n")
cat("\n--------------------\n")
print(psis[1:24])
#Model expressed as an AR infinity (pi-weights)
pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
names(pis)=paste("pi",1:36)
cat("\nPi-weights (AR(inf))\n")
cat("\n--------------------\n")
print(pis[1:24])
cat("\nDescriptive Statistics for the Residuals\n")
cat("\n----------------------------------------\n")
suppressMessages(require(fBasics,quietly=TRUE,warn.conflicts=FALSE))
##Anderson-Darling test
print(basicStats(resid))
## Add here complementary tests (use with caution!)
##---------------------------------------------------------
cat("\nNormality Tests\n")
cat("\n--------------------\n")
##Shapiro-Wilks Normality test
print(shapiro.test(resid))
suppressMessages(require(nortest,quietly=TRUE,warn.conflicts=FALSE))
##Anderson-Darling test
print(ad.test(resid))
suppressMessages(require(tseries,quietly=TRUE,warn.conflicts=FALSE))
##Jarque-Bera test
print(jarque.bera.test(resid))
cat("\nHomoscedasticity Test\n")
cat("\n--------------------\n")
suppressMessages(require(lmtest,quietly=TRUE,warn.conflicts=FALSE))
##Breusch-Pagan test
obs=get(model$series)
print(bptest(resid~I(obs-resid)))
cat("\nIndependence Tests\n")
cat("\n--------------------\n")
##Durbin-Watson test
print(dwtest(resid~I(1:length(resid))))
##Ljung-Box test
cat("\nLjung-Box test\n")
print(t(apply(matrix(c(1:4,(1:4)*s)),1,function(el) {
te=Box.test(resid,type="Ljung-Box",lag=el)
c(lag=(te$parameter),statistic=te$statistic[[1]],p.value=te$p.value)})))
#Extra Alexis
# Calculate T-ratios: Coefficient / Standard Error
t_ratios <- model$coef / sqrt(diag(model$var.coef))
# Create a summary table
results <- data.frame(
Coeff = round(model$coef, 4),
SE = round(sqrt(diag(model$var.coef)), 4),
t_ratio = round(t_ratios, 4),
Significant = abs(t_ratios) > 2 # Significance threshold
)
print(results)
cat("\nSigma^2 (Variance of Residuals):", model$sigma2)
cat("\nAIC:", AIC(model))
cat("\nBIC:", BIC(model))
# End
}
################# Fi Validation #################################
validation(mod1)
# --- 1. Regular AR(2) Part Analysis ---
# Equation: (1 - phi1*B - phi2*B^2) X_t = ...
phi1 <- coef(mod1)["ar1"]
phi2 <- coef(mod1)["ar2"]
cat("\n--- AR(2) Characteristic Polynomial ---\n")
cat("Equation: 1 -", round(phi1, 4), "* B -", round(phi2, 4), "* B^2 = 0\n")
# Calculate Roots
ar_roots <- polyroot(c(1, -phi1, -phi2))
cat("\nRoots of AR Polynomial:\n")
print(ar_roots)
# Check Moduli (Must be > 1 for Stationarity)
cat("\nModuli of AR Roots (> 1 means Stationary/Causal):\n")
print(Mod(ar_roots))
# --- 2. Seasonal MA(3) Part Analysis ---
# Equation: ... = (1 + theta1*Z + theta2*Z^2 + theta3*Z^3) E_t
# Where Z = B^12 (Seasonal Backshift)
theta1 <- coef(mod1)["sma1"]
theta2 <- coef(mod1)["sma2"]
theta3 <- coef(mod1)["sma3"]
cat("\n--- Seasonal MA(3) Characteristic Polynomial (in Z = B^12) ---\n")
cat("Equation: 1 +", round(theta1, 4), "* Z +", round(theta2, 4), "* Z^2 +", round(theta3, 4), "* Z^3 = 0\n")
# Calculate Roots (in terms of Z)
sma_roots <- polyroot(c(1, theta1, theta2, theta3))
cat("\nRoots of Seasonal MA Polynomial:\n")
print(sma_roots)
# Check Moduli (Must be > 1 for Invertibility)
cat("\nModuli of Seasonal MA Roots (> 1 means Invertible):\n")
print(Mod(sma_roots))
ultim=c(2018,12)
pdq=c(2,0,0)
PDQ=c(0,1,3)
serie2=window(serie,end=ultim)
lnserie2=log(serie2)
serie1=window(serie,end=ultim+c(1,0))
lnserie1=log(serie1)
(modA=arima(lnserie1,order=pdq,seasonal=list(order=PDQ,period=12)))
(modB=arima(lnserie2,order=pdq,seasonal=list(order=PDQ,period=12)))
check_stability_prediction <- function(serie, pdq, PDQ, ultim) {
# 1. Setup Data Subsets
# Full Series (includes test year)
serie1 <- window(serie, end=ultim + c(1, 0))
lnserie1 <- log(serie1)
# Training Series (ends at cut-off)
serie2 <- window(serie, end=ultim)
lnserie2 <- log(serie2)
# 2. Stability Check (Compare Coefficients)
cat("\n================ STABILITY CHECK ================\n")
cat("Model Structure: ARIMA(", paste(pdq, collapse=","), ")(", paste(PDQ, collapse=","), ")12\n")
cat("\n--- Model A: Full Data (1990-2019) ---\n")
modA <- arima(lnserie1, order=pdq, seasonal=list(order=PDQ, period=12))
print(round(modA$coef, 4))
cat("\n--- Model B: Training Data (1990-2018) ---\n")
modB <- arima(lnserie2, order=pdq, seasonal=list(order=PDQ, period=12))
print(round(modB$coef, 4))
# 3. Forecasting (One year ahead)
pred <- predict(modB, n.ahead=12)
# Construct plot objects
pr <- ts(c(tail(lnserie2, 1), pred$pred), start=ultim, freq=12)
se <- ts(c(0, pred$se), start=ultim, freq=12)
# Back-transform confidence intervals
tl <- ts(exp(pr - 1.96 * se), start=ultim, freq=12)
tu <- ts(exp(pr + 1.96 * se), start=ultim, freq=12)
pr <- ts(exp(pr), start=ultim, freq=12)
# 4. Plotting
ts.plot(serie, tl, tu, pr,
lty=c(1, 2, 2, 1),
col=c("black", "blue", "blue", "red"),
xlim=ultim[1] + c(-2, +2),
ylab="GwH",
main=paste("Predictive Capability: ARIMA(", paste(pdq, collapse=","), ")(", paste(PDQ, collapse=","), ")12", sep=""))
abline(v=(ultim[1]-2):(ultim[1]+2), lty=3, col="gray")
legend("topleft", legend=c("Actual Data", "Forecast", "95% CI"),
col=c("black", "red", "blue"), lty=c(1, 1, 2), bty="n", cex=0.8)
# 5. Accuracy Metrics
obs <- window(serie, start=ultim + c(0, 1), end=ultim + c(1, 0))
pr_val <- window(pr, start=ultim + c(0, 1))
rmse <- sqrt(mean((obs - pr_val)^2))
mape <- mean(abs((obs - pr_val) / obs)) * 100
cat("\n================ ACCURACY METRICS (2019) ================\n")
cat("RMSE (Root Mean Sq Error):", round(rmse, 2), "GwH\n")
cat("MAPE (Mean Abs % Error):  ", round(mape, 2), "%\n")
}
# Define your cut-off date (Dec 2018)
cutoff_date <- c(2018, 12)
# Run for Model 1: ARIMA(2,0,0)(0,1,3)12
check_stability_prediction(serie, pdq=c(2,0,0), PDQ=c(0,1,3), ultim=cutoff_date)
validation(mod2)
# --- 1. Regular AR(2) Part Analysis ---
# Checks for Regular Stationarity (Causality)
# Equation: (1 - phi1*B - phi2*B^2) = 0
phi1 <- coef(mod2)["ar1"]
phi2 <- coef(mod2)["ar2"]
cat("\n--- Regular AR(2) Characteristic Polynomial ---\n")
cat("Equation: 1 -", round(phi1, 4), "* B -", round(phi2, 4), "* B^2 = 0\n")
# Calculate Roots
ar_roots <- polyroot(c(1, -phi1, -phi2))
cat("Moduli of Regular AR Roots (> 1 means Stationary):\n")
print(Mod(ar_roots))
# --- 2. Seasonal AR(3) Part Analysis ---
# Checks for Seasonal Stationarity (Causality)
# Equation: (1 - Phi1*Z - Phi2*Z^2 - Phi3*Z^3) = 0  (Where Z = B^12)
Phi1 <- coef(mod2)["sar1"]
Phi2 <- coef(mod2)["sar2"]
Phi3 <- coef(mod2)["sar3"]
cat("\n--- Seasonal AR(3) Characteristic Polynomial (in Z = B^12) ---\n")
cat("Equation: 1 -", round(Phi1, 4), "* Z -", round(Phi2, 4), "* Z^2 -", round(Phi3, 4), "* Z^3 = 0\n")
# Calculate Roots (in terms of Z)
# Note: We use negative signs for AR coefficients in the polynomial
sar_roots <- polyroot(c(1, -Phi1, -Phi2, -Phi3))
cat("Moduli of Seasonal AR Roots (> 1 means Stationary):\n")
print(Mod(sar_roots))
ultim=c(2018,12)
pdq=c(2,0,0)
PDQ=c(3,1,0)
serie2=window(serie,end=ultim)
lnserie2=log(serie2)
serie1=window(serie,end=ultim+c(1,0))
lnserie1=log(serie1)
(modA=arima(lnserie1,order=pdq,seasonal=list(order=PDQ,period=12)))
(modB=arima(lnserie2,order=pdq,seasonal=list(order=PDQ,period=12)))
# Define your cut-off date (Dec 2018)
cutoff_date <- c(2018, 12)
# Run for Model 2: ARIMA(2,0,0)(3,1,0)12
check_stability_prediction(serie, pdq=c(2,0,0), PDQ=c(3,1,0), ultim=cutoff_date)
# --- 1. Helper Function to Calculate Metrics ---
get_model_metrics <- function(model_obj, data_series, cutoff=c(2018,12)) {
# Split data
train_data <- window(data_series, end=cutoff)
actual_2019 <- window(data_series, start=cutoff+c(0,1), end=cutoff+c(1,0))
# Extract structure from the input model object
orders <- model_obj$arma[c(1, 6, 2)] # p, d, q
season <- model_obj$arma[c(3, 7, 4)] # P, D, Q
period <- model_obj$arma[5]
# Re-estimate on training data
mod_train <- arima(log(train_data), order=orders, seasonal=list(order=season, period=period))
pred <- predict(mod_train, n.ahead=12)
# Forecast values (Back-transformed)
fc_mean <- exp(pred$pred)
fc_low  <- exp(pred$pred - 1.96 * pred$se)
fc_high <- exp(pred$pred + 1.96 * pred$se)
# Calculate Metrics
rmse  <- sqrt(mean((actual_2019 - fc_mean)^2))
mae   <- mean(abs(actual_2019 - fc_mean))
rmspe <- sqrt(mean(((actual_2019 - fc_mean)/actual_2019)^2))
mape  <- mean(abs((actual_2019 - fc_mean)/actual_2019)) * 100
mean_len_ci <- mean(fc_high - fc_low)
return(c(RMSE=rmse, MAE=mae, RMSPE=rmspe, MAPE=mape, MeanCI=mean_len_ci))
}
# --- 2. Calculate Scores ---
# Ensure mod1 and mod2 are the correct models you want to compare
m1_scores <- get_model_metrics(mod1, serie)
m2_scores <- get_model_metrics(mod2, serie)
# --- 3. Create Dataframe (Numeric Part First) ---
resul_num <- data.frame(
Parameters = c(length(coef(mod1)), length(coef(mod2))),
Sigma2     = c(mod1$sigma2, mod2$sigma2),
AIC        = c(AIC(mod1), AIC(mod2)),
BIC        = c(BIC(mod1), BIC(mod2)),
RMSE       = c(m1_scores["RMSE"], m2_scores["RMSE"]),
MAE        = c(m1_scores["MAE"], m2_scores["MAE"]),
RMSPE      = c(m1_scores["RMSPE"], m2_scores["RMSPE"]),
MAPE       = c(m1_scores["MAPE"], m2_scores["MAPE"]),
Mean_CI    = c(m1_scores["MeanCI"], m2_scores["MeanCI"])
)
# --- 4. Round Numbers to Strings ---
# This makes the table pretty (e.g., "1.57" instead of "1.5738292")
resul_clean <- format(round(resul_num, 4), nsmall=4)
# --- 5. Add Text Columns & Row Names ---
resul_clean$Homoscedasticity <- c("Validated", "Validated")
resul_clean$Normality        <- c("Validated", "Validated")
resul_clean$Independence     <- c("Fail (<0.05)", "Fail (<0.05)")
resul_clean$Causality        <- c("Validated", "Validated")
resul_clean$Invertibility    <- c("Validated", "Validated")
resul_clean$Stability        <- c("Validated", "Validated")
resul_clean$Predictive_capability <- c("Validated", "Validated")
# Set Model Names
row.names(resul_clean) <- c("Model 1: (2,0,0)(0,1,3)12", "Model 2: (2,0,0)(3,1,0)12")
# --- 6. Pivot (Transpose) ---
# This flips the table and converts it to a matrix of strings
final_table <- t(resul_clean)
# Print the final result
print(final_table, quote=FALSE)
# --- Section 4: Final Prediction (Model 1) ---
# Model: ARIMA(2,0,0)(0,1,3)12
pdq <- c(2, 0, 0)
PDQ <- c(0, 1, 3)
# 1. Estimate on the FULL available history (1990-2019)
# We use the log-transformed series 'lnserie'
final_mod <- arima(lnserie,
order = pdq,
seasonal = list(order = PDQ, period = 12))
cat("\n--- Final Model Estimation (Full History) ---\n")
print(final_mod)
# 2. Forecast the Future (12 months ahead)
# This predicts Jan-Dec 2020
future_pred <- predict(final_mod, n.ahead = 12)
# 3. Process the Results
# Get the end date of your current series
last_date <- end(serie)
# Create Time Series for the Forecast (Log Scale)
pr <- ts(future_pred$pred, start = last_date + c(0, 1), freq = 12)
se <- ts(future_pred$se, start = last_date + c(0, 1), freq = 12)
# Back-transform to Original Scale (GwH) using exp()
pred_mean <- exp(pr)                # Point Forecast
pred_low  <- exp(pr - 1.96 * se)    # Lower 95% Confidence Interval
pred_high <- exp(pr + 1.96 * se)    # Upper 95% Confidence Interval
# 4. Visualization
# We plot the whole series + forecast, zooming in on the last 5 years
ts.plot(serie, pred_mean, pred_low, pred_high,
lty = c(1, 1, 2, 2),
col = c("black", "red", "blue", "blue"),
xlim = c(2015, 2021),  # Zoom to see details
ylab = "GwH",
main = "Final Forecast 2020: ARIMA(2,0,0)(0,1,3)12")
# Add grid and legend
abline(v = 2015:2021, lty = 3, col = "gray")
legend("topleft", legend = c("Historical Data", "Forecast", "95% CI"),
col = c("black", "red", "blue"), lty = c(1, 1, 2), bty = "n", cex=0.8)
# 5. Print the Numerical Forecasts (Table for Report)
cat("\n--- Forecast Values (GwH) for 2020 ---\n")
forecast_table <- data.frame(
Month = time(pred_mean),
Lower_95 = round(pred_low, 2),
Forecast = round(pred_mean, 2),
Upper_95 = round(pred_high, 2)
)
print(forecast_table)
plot(d12lnserie)
plot(d12lnserie, main="d12lnserie")
par(mfrow=c(1,2))
acf(d12lnserie,ylim=c(-1,1),col=c(2,rep(1,11)),lwd=2,lag.max=72, main="ACF of d12lnserie")
pacf(d12lnserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=72, main="PACF of d12lnserie")
par(mfrow=c(1,1))
par(mfrow=c(1,2))
acf(d12lnserie,ylim=c(-1,1),col=c(2,rep(1,11)),lwd=2,lag.max=72, main="ACF of d12lnserie")
pacf(d12lnserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=72, main="PACF of d12lnserie")
par(mfrow=c(1,1))
